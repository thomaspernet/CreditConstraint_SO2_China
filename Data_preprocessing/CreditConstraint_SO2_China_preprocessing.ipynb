{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Credit constraint and pollution\n",
        "\n",
        "In this notebook, we are preparing the dataset for the paper on financial constraints and emision of SO2 in China. \n",
        "\n",
        "The related documentation about the paper can be found [here](https://coda.io/d/Credit-pollution_dc0UH1KLdc5/Ressources_suqer#_luHVD)\n",
        "\n",
        "## Data Source\n",
        "  \n",
        "1. Credit Constraint data source. \n",
        "\n",
        "The data comes from the following paper: Credit constraints, quality, and export prices: Theory and evidence from China, from Fan et al.\n",
        "\n",
        "- [Spreadsheet credit_constraint_Manova/Poncet](https://docs.google.com/spreadsheets/d/1QJRCSlddFFjguGIHkmY67m82lJkMRkcndbCbmlWfL2Y/edit)\n",
        "    - Sheet: Fan_Chinese_data\n",
        "    - [Original source](https://docs.google.com/file/d/1vPQGz1FVQ6QDUqj60YBijDBeEo5iMQu_/edit)\n",
        "    \n",
        "2. Pollution Data\n",
        "\n",
        "The data comes from plant-level pollution data from the Chinese Environmental Statistical Database (CESD) provided by the Ministry of Environmental Protection (MEP)\n",
        "\n",
        "- [Spreadsheet pollution_city_cic4_china](https://docs.google.com/spreadsheets/d/16QX2e0rHbYzkevV7H0cwYwRDMUhZ2m1e6-dQ5ENUaYg/edit?usp=drive_web&ouid=100006339688710166679)\n",
        "\n",
        "3. Load cities from the literature\n",
        "\n",
        "We keep only the cities used in the litterature and that we can add extra data. We use a consistent spreadsheet to make sure all the city codes are used.\n",
        "\n",
        "- [Spreadsheet cityname_and_code](https://docs.google.com/spreadsheets/d/1fIziz-Xt99-Rj6NLm52-i6jScOLXgAY20KJi8k3DruA/edit#gid=140212755)\n",
        "    - Sheet: final\n",
        "    \n",
        "4. Province Pollution objective\n",
        "\n",
        "The data comes from the following paper: Environmental regulation and firm exports: Evidence from the eleventh Five-Year Plan in China, from Shi et al.\n",
        "\n",
        "The data have been adjusted to get the correct objective by SO2 and COD. The adjusted data comes from \n",
        "\n",
        "- [Spreadsheet FYP_reduction_target](https://docs.google.com/spreadsheets/d/1dMy_ht-aLjYYGGWGsK7IrL5mkmFh3XxlYTS2qNGnYXw/edit#gid=1428462130)\n",
        "    - Sheet: target_SO2\n",
        "    -[Original source](https://drive.google.com/open?id=1bONlgotKRtNcX8yPNJ_iamw6NviFLRxN)\n",
        "    \n",
        "5. Distance between each city in China\n",
        "\n",
        "To construct the distance, we use the longitude and latitude and then use the Haversine formula. The notebook is at this [URL](https://drive.google.com/open?id=1Bp60rL-QjL6_tIB6yLwVvHMXti2mZAsJ)\n",
        "\n",
        "- [Spreadsheet China_cities_distance.csv](https://docs.google.com/spreadsheets/d/1nFSg83a0P-XMXEgP58HD0RIZ-qV3ZafOwZ1jqi6EFHk/edit#gid=1101625944)\n",
        "\n",
        "6. ASIF Dataset\n",
        "\n",
        "The information is generated from the Annual\n",
        "Survey of Industrial Firms (ASIF) conducted by China's National Bureau of Statistics (NBS) for the period from 2002 to 2007. \n",
        "\n",
        "The ASIF contains basic firm information (name, address, industry classification, ownership, etc.) and major financial variables (annual output value, annual sales, assets, etc.)\n",
        "\n",
        "- Data store in big query, available from request\n",
        "\n",
        "\n",
        "## Pipeline\n",
        "\n",
        "In this section, we describe the operation to follow in order to fully process the datasets to get a clean and workable dataset\n",
        "\n",
        "### Step 1:   Aggregate CIC 2\n",
        "\n",
        "Since the financial constraint variable is at the CIC 2 digit level, we need to aggregate the pollution data at the same level\n",
        "\n",
        "### Step 2: Merge dataset\n",
        "\n",
        "We merge the pollution data with the credit dataset and the provinces objectives. \n",
        "\n",
        "### Step 3: Create additional variables\n",
        "\n",
        "First of all, we need to compute the reduction mandate at the city level following the paper of Chen Zhao, The consequences of spatially differentiated water pollution regulation in China. \n",
        "\n",
        "The data is stored in this [spreadsheet](https://docs.google.com/spreadsheets/d/1z3A_I8_StdyNL5O38s2l9hx6W3VR49CGVmaosypjFMA/edit#gid=550420287).\n",
        "\n",
        "Next, we compute numerous metrics to capture the spatial realocation of the activities. We finaly retain one candidate, defined as the reduction mandate city i - average reduction cities j over output city i - average citiesj. \n",
        "\n",
        "All the metrics are available in the previous spreadsheet, `China_cities_target_so2.csv`\n",
        "\n",
        "### Step 4: Finalize dataset\n",
        "\n",
        "The most important step is to combine the pollution/credit data with the various additional data. At the end of the day, the final dataset contains 25,404 observations and 20 variables. \n",
        "\n",
        "A brief statistic descriptive is available below:\n",
        "\n",
        "|  index | year               | SO2_emissions_2005   | ttoutput           | tso2               | so2_intensity_value_added    | tso2_mandate_c       | avg_ij_o_city_mandate     | fixed_asset        | employment         | FE_c_y            | FE_c_i             | FE_i_y             |\n",
        "| ------ | ------------------ | -------------------- | ------------------ | ------------------ | ---------------------------- | -------------------- | ------------------------- | ------------------ | ------------------ | ----------------- | ------------------ | ------------------ |\n",
        "| count  | 25404.0            | 25404.0              | 25404.0            | 25404.0            | 25404.0                      | 25404.0              | 25404.0                   | 25404.0            | 25404.0            | 25404.0           | 25404.0            | 25404.0            |\n",
        "| mean   | 2004.4866556447803 | 109.47135490473741   | 93096.56080546095  | 1061859.0441269092 | 16.73469569644443            | 0.12447486957113552  | 0.0011241788415940954     | 703193.7771217132  | 8756.609707132735  | 803.1306487167375 | 3003.0344040308614 | 89.8419146591088   |\n",
        "| std    | 1.7129850690687007 | 46.49644132847797    | 152639.07555371244 | 3896730.941290046  | 747.6616361298               | 0.1609689788153621   | 0.06531571638765048       | 1293704.8691019986 | 17566.496078666398 | 468.0144201999909 | 1753.0233472148748 | 47.414186394153255 |\n",
        "| min    | 2002.0             | 2.2                  | 132.0              | 1.0                | 1.3926121923197437e-06       | 0.0                  | -0.6758034750737683       | 4.0                | 3.0                | 0.0               | 1.0                | 0.0                |\n",
        "| 25%    | 2003.0             | 61.3                 | 7317.75            | 21689.75           | 0.11728907561326757          | 0.026132208255912493 | 0.00013418264618065052    | 83142.5            | 1305.0             | 420.0             | 1475.0             | 53.0               |\n",
        "| 50%    | 2004.0             | 119.7                | 29362.4            | 107757.5           | 0.586647092145884            | 0.07454041757039813  | 0.00302789464824566       | 259361.0           | 3517.5             | 802.0             | 3011.5             | 94.0               |\n",
        "| 75%    | 2006.0             | 137.3                | 102053.25          | 526100.0           | 2.695841348726034            | 0.1608424380370474   | 0.00623644617529759       | 753923.25          | 9010.0             | 1186.0            | 4527.25            | 128.0              |\n",
        "| max    | 2007.0             | 200.3                | 892501.0           | 148754812.0        | 108000.0                     | 1.3279082279578365   | 0.4123807207097676        | 30376774.0         | 541735.0           | 1682.0            | 6056.0             | 173.0              |\n",
        "\n"
      ],
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "heading_collapsed": true,
        "id": "ley4g-yEWVe8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset preparation"
      ],
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "id": "ZZrEalchXmne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from GoogleDrivePy.google_authentification import connect_service_local\n",
        "from GoogleDrivePy.google_drive import connect_drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
        "plt.rcParams['font.serif'] = ['SimHei']\n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\",{\"font.sans-serif\":['simhei', 'Arial']})\n",
        "\n",
        "\n",
        "pathcredential = '/Users/Thomas/Google Drive/Projects/Data_science/' \\\n",
        "'Google_code_n_Oauth/Client_Oauth/Google_auth/'\n",
        "scopes = ['https://www.googleapis.com/auth/documents.readonly',\n",
        "            'https://www.googleapis.com/auth/drive', \n",
        "         'https://www.googleapis.com/auth/spreadsheets.readonly']\n",
        "\n",
        "serviceaccount = '/Users/Thomas/Google Drive/Projects/Data_science/' \\\n",
        "'Google_code_n_Oauth/Client_Oauth/Google_auth/valid-pagoda-132423-c6ac84b41833.json'\n",
        "\n",
        "cs = connect_service_local.connect_service_local(path_json =pathcredential,\n",
        "                                                 path_service_account = serviceaccount,\n",
        "                                                 scope = scopes)\n",
        "service = cs.get_service()\n",
        "cdr = connect_drive.connect_drive(service)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "t04avwoYX2j-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1: Import credit constraint\n",
        "\n",
        "The credit constraint is computed for 29 CIC sectors:\n",
        "\n",
        "|  CIC | Industry Name                                                                      | financial_dep_china   |\n",
        "| ---- | ---------------------------------------------------------------------------------- | --------------------- |\n",
        "| 35   | General Purpose Machinery                                                          | -2.59                 |\n",
        "| 16   | Tobacco                                                                            | -1.54                 |\n",
        "| 41   | Measuring Instruments and Machinery forCultural Activity and Office Work           | -1.34                 |\n",
        "| 18   | Textile Wearing Apparel, Footwear, and Caps                                        | -1.32                 |\n",
        "| 19   | Leather, Fur, Feather and Related Products                                         | -1.11                 |\n",
        "| 34   | Metal Products                                                                     | -0.93                 |\n",
        "| 23   | Printing, Reproduction of Recording Media                                          | -0.8                  |\n",
        "| 15   | Beverages                                                                          | -0.72                 |\n",
        "| 20   | Processing of Timber, Manufacture of Wood,Bamboo, Rattan, Palm, and Straw Products | -0.72                 |\n",
        "| 37   | Transport Equipment                                                                | -0.72                 |\n",
        "| 21   | Furniture                                                                          | -0.65                 |\n",
        "| 42   | Artwork and Other Manufacturing                                                    | -0.62                 |\n",
        "| 17   | Textile                                                                            | -0.48                 |\n",
        "| 13   | Processing of Food from Agricultural Products                                      | -0.47                 |\n",
        "| 30   | Plastics                                                                           | -0.47                 |\n",
        "| 27   | Medicines                                                                          | -0.44                 |\n",
        "| 39   | Electrical Machinery and Equipment                                                 | -0.44                 |\n",
        "| 28   | Chemical Fibers                                                                    | -0.41                 |\n",
        "| 24   | Articles For Culture, Education and Sport Activity                                 | -0.4                  |\n",
        "| 14   | Foods                                                                              | -0.32                 |\n",
        "| 31   | Non-metallic Mineral Products                                                      | -0.29                 |\n",
        "| 36   | Special Purpose Machinery                                                          | -0.27                 |\n",
        "| 29   | Rubber                                                                             | -0.26                 |\n",
        "| 26   | Raw Chemical Materials and Chemical Products                                       | -0.23                 |\n",
        "| 33   | Smelting and Pressing of Non-ferrous Metals                                        | -0.1                  |\n",
        "| 40   | Communication Equipment, Computers and Other Electronic Equipment                  | 0.02                  |\n",
        "| 22   | Paper and Paper Products                                                           | 0.07                  |\n",
        "| 32   | Smelting and Pressing of Ferrous Metals                                            | 0.33                  |\n",
        "| 25   | Processing of Petroleum, Coking, Processing of Nuclear Fuel                        | 0.62                  |"
      ],
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "id": "Sh60EQydYXFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sheetid = '1QJRCSlddFFjguGIHkmY67m82lJkMRkcndbCbmlWfL2Y'\n",
        "range_name = 'Fan_Chinese_data!A1:H30'\n",
        "\n",
        "credit_china = service['sheet'].spreadsheets().values().get(\n",
        "    spreadsheetId= sheetid,\n",
        "    range=range_name).execute()\n",
        "credit_china = pd.DataFrame(credit_china.get('values', []), columns = \n",
        "                        credit_china['values'][0]).drop([0])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "m4O0sEd4WSyf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2: import pollution\n",
        "\n",
        "We import the data for year 2002 to 2007. The total number of city is 362, 1825 CIC 4 digits and  31 2 digits. In the next step, we will use a restricted number of cities and 2 digits CIC.\n",
        "\n",
        "|  Year    |  Mean SO2       |\n",
        "| -------- | --------------- |\n",
        "| 1998     |   388228.947276 |\n",
        "| 1999     |   229527.169066 |\n",
        "| 2000     |   238108.650460 |\n",
        "| 2001     |  265766.673305  |\n",
        "| 2002     |  268103.305621  |\n",
        "| 2003     | 277024.498384   |\n",
        "| 2004     | 329237.209233   |\n",
        "| 2005     | 370408.708490   |\n",
        "| 2006     | 360947.630981   |\n",
        "| 2007     | 331399.635638   |"
      ],
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "id": "MdVsL70nY_sm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_col = ['year','citycode','prov2013','pref2013','indus_code','ind2',\n",
        "            'ttoutput','twaste_water','tCOD','tAmmonia_Nitrogen','twaste_gas',\n",
        "            'tso2','tNOx','tsmoke_dust','tsoot']\n",
        "\n",
        "sheetid = '16QX2e0rHbYzkevV7H0cwYwRDMUhZ2m1e6-dQ5ENUaYg'\n",
        "range_name = 'pollution_city_cic4_china.csv!A2:O205606'\n",
        "\n",
        "pollution = service['sheet'].spreadsheets().values().get(\n",
        "    spreadsheetId= sheetid,\n",
        "    range=range_name).execute()\n",
        "pollution = pd.DataFrame(pollution.get('values', []),\n",
        "                              columns=list_col)\n",
        "pollution.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "CHAg8J2LY-xn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pollution.shape"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process pollution\n",
        "\n",
        "In this step, we need to convert the variable to the appropriate format and keep the relevant variables"
      ],
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "id": "zSKSOzrWVYcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pollution['year'] = pollution['year'].astype('int')\n",
        "df_pollution = pollution[pollution['year'] > 2001]\n",
        "df_pollution = df_pollution[df_pollution['prov2013'] != '']\n",
        "\n",
        "\n",
        "list_to_numeric = ['ttoutput', 'twaste_water', 'tCOD', 'tAmmonia_Nitrogen',\n",
        "                   'twaste_gas', 'tso2', 'tNOx', 'tsmoke_dust', 'tsoot']\n",
        "for name in list_to_numeric:\n",
        "    df_pollution[name] = pd.to_numeric(df_pollution[name])\n",
        "  \n",
        "df_pollution = df_pollution.drop(columns = [\n",
        "                                      'pref2013',\n",
        "                                      'tAmmonia_Nitrogen',\n",
        "                                      'twaste_gas',\n",
        "                                     'tNOx', \n",
        "                                     'tsmoke_dust',\n",
        "                                     'tsoot',\n",
        "'twaste_water'])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "JTLjXUQHcHU1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot pollution all sample\n",
        "\n",
        "Below, we create a simple function to plot the sum and mean of SO2/CO2 by year. It helps us to see if the trend is the same after we remove some observations"
      ],
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "id": "e30a70qawwtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_ts_pollution(l_df, step = 1, move_to_drive = False):\n",
        "    \"\"\"\n",
        "      Plot the sum and mean of SO2 and COD by year\n",
        "    \"\"\"\n",
        "  \n",
        "  ###\n",
        "    fig, axarr = plt.subplots(2, 2, figsize=(12, 8))\n",
        "    for df_temp in l_df:\n",
        "        temp = df_temp.groupby(['year']).agg({'tCOD':['sum', 'mean'],\n",
        "                                    'tso2':['sum', 'mean']})\n",
        "        temp.columns = temp.columns.droplevel()\n",
        "        temp.columns = ['tCOD_sum', 'tCOD_mean', 'tso2_sum', 'tso2_mean']\n",
        "\n",
        "        \n",
        "        g = sns.lineplot(y=\"tCOD_sum\", x= temp.index, data=temp , ax=axarr[0][0])\n",
        "        g = sns.lineplot(y=\"tCOD_mean\", x= temp.index, data=temp, ax=axarr[0][1])\n",
        "        g = sns.lineplot(y=\"tso2_sum\", x= temp.index, data=temp,  ax=axarr[1][0])\n",
        "        g = sns.lineplot(y=\"tso2_mean\", x= temp.index, data=temp, ax=axarr[1][1])\n",
        "    fig.suptitle('Mean & Sum of COD/SO2 step ' + str(step))\n",
        "    \n",
        "    \n",
        "    if move_to_drive:\n",
        "        name = 'year_pollution_step' + str(step) + '.png'\n",
        "        g.get_figure().savefig(name)\n",
        "        mime_type = \"image/png\"\n",
        "        cdr.upload_file_root(mime_type, name)\n",
        "        cdr.move_file(file_name = name,\n",
        "                    folder_name = 'DataPreparation_pollution')\n",
        "        os.remove(name)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "wvxpQCaU327m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot"
      ],
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "id": "QXJ2KPcWuF9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " plot_ts_pollution(l_df = [df_pollution],\n",
        "                   step = 1,\n",
        "                   move_to_drive = True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "akc088wd5vy_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load city from the litterature\n",
        "\n",
        "We load a consistent spreadsheet with the city name and city codes. Some cities like Beijing, Shanghai or Tianjin have more than one city codes over time. \n",
        "\n",
        "In total, there are 285 cities"
      ],
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "id": "JA1FMbMca031"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sheetid = '1fIziz-Xt99-Rj6NLm52-i6jScOLXgAY20KJi8k3DruA'\n",
        "range_name = 'final!A1:C293'\n",
        "\n",
        "city_name = service['sheet'].spreadsheets().values().get(\n",
        "    spreadsheetId= sheetid,\n",
        "    range=range_name).execute()\n",
        "city_name = pd.DataFrame(city_name.get('values', []), columns = \n",
        "                        city_name['values'][0]).drop([0])\n",
        "city_name.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "32itRUFLa24U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can merge the cities with the pollution data"
      ],
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "scOF1NOFCVsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_final_credit = pd.merge(df_pollution,\n",
        "                           city_name,\n",
        "                           how = 'inner',\n",
        "                           left_on = 'citycode',\n",
        "                           right_on = 'geocode4_corr'\n",
        "        )\n",
        "df_final_credit.shape"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "E7C2fIxFcaBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_final_credit.shape"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "w9kWYRD_cccH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4: Import FYP data\n",
        "\n",
        "We load the 11th five year plan data containing the SO2/COD objective. The governement set a national target of 10% SO2 decreased by 2010. This 10% is spread over 28 provinces, with different objectives. \n",
        "\n",
        "|  Province    | province_ch  | Target_Reduction_SO2   |\n",
        "| ------------ | ------------ | ---------------------- |\n",
        "| Shandong     | 山东省          | 40.1                   |\n",
        "| Jiangsu      | 江苏省          | 24.7                   |\n",
        "| Henan        | 河南省          | 22.8                   |\n",
        "| Hebei        | 河北省          | 22.5                   |\n",
        "| Shanxi       | 山西省          | 21.2                   |\n",
        "| Guizhou      | 贵州省          | 20.4                   |\n",
        "| Zhejiang     | 浙江省          | 19.4                   |\n",
        "| Chongqing    | 重庆市          | 15.5                   |\n",
        "| Shaanxi      | 陕西省          | 14.4                   |\n",
        "| Shanghai     | 上海市          | 13.3                   |\n",
        "| Guangdong    | 广东省          | 12.9                   |\n",
        "| Liaoning     | 辽宁省          | 11.1                   |\n",
        "| Guangxi      | 广西壮族自治区      | 10.1                   |\n",
        "| Sichuan      | 四川省          | 10                     |\n",
        "| Hunan        | 湖南省          | 8.3                    |\n",
        "| Hubei        | 湖北省          | 5.6                    |\n",
        "| Neimenggu    | 内蒙古自治区       | 5.6                    |\n",
        "| Jiangxi      | 江西省          | 4.3                    |\n",
        "| Beijing      | 北京市          | 3.9                    |\n",
        "| Fujian       | 福建省          | 3.7                    |\n",
        "| Ningxia      | 宁夏回族自治区      | 3.2                    |\n",
        "| Tianjin      | 天津市          | 2.5                    |\n",
        "| Anhui        | 安徽省          | 2.3                    |\n",
        "| Yunan        | 云南省          | 2.1                    |\n",
        "| Jilin        | 吉林省          | 1.8                    |\n",
        "| Heilongjiang | 黑龙江省         | 1                      |\n",
        "| Guansu       | 甘肃省          | 0                      |\n",
        "| Xinjiang     | 新疆维吾尔自治区     | 0                      |\n",
        "| Qinghai      | 青海省          | 0                      |\n",
        "| Hainan       | 海南省          | 0                      |\n",
        "| Tibet        | 西藏壮族自治区      | 0                      |\n",
        "\n",
        "The national average is 9.66\n",
        "\n",
        "| Location | Percentage change |\n",
        "| -------- | ----------------- |\n",
        "| East     | 0.137965          |\n",
        "| Central  | 0.093166          |\n",
        "| West     | 0.060009          |"
      ],
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "id": "ZBHgGNnZRduA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sheetid = '1dMy_ht-aLjYYGGWGsK7IrL5mkmFh3XxlYTS2qNGnYXw'\n",
        "range_name = 'target_SO2!A1:K32'\n",
        "\n",
        "fyp = service['sheet'].spreadsheets().values().get(\n",
        "    spreadsheetId= sheetid,\n",
        "    range=range_name).execute()\n",
        "fyp = pd.DataFrame(fyp.get('values', []), columns = \n",
        "                        fyp['values'][0]).drop([0])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "EkU4A8F50mzc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline\n",
        "\n",
        "## Step 1: Aggregate CIC 2\n",
        "\n",
        "The first steps consists to aggregate the 4 digits CIC to 2 digits CIC. We start with 128039 observations and after we get 31439 obs.\n",
        "\n",
        "Our three variables of interests are `ttoutput`, `tCOD` and `tso2`\n",
        "\n",
        "We have now 285 cities\n",
        "\n"
      ],
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "id": "K0e4V2oYcWri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_final_credit.shape"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "DvqsDfnW1-rq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pollution_ind2 = df_final_credit.groupby(['year',\n",
        "                                          'geocode4_corr',\n",
        "                                          'citycn',\n",
        "                                          'prov2013',\n",
        "                                          'ind2']).sum().reset_index()\n",
        "df_pollution_ind2.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "dBX4MXeqK1D6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_pollution_ind2['citycn'].unique())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "K4jqwRbcc40B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_ts_pollution(l_df = [df_pollution, df_pollution_ind2],\n",
        "                   step = 2,\n",
        "                   move_to_drive  = False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "xbXdA3an84BS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pollution_ind2['ind2'].sort_values().unique()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "KPsiyauneIyh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Merge Datasets\n",
        "\n",
        "Now that we are at the right industry level, we can merge the datasets. We start with 31439 observations and we end with .\n",
        "\n",
        "\n",
        "\n",
        "### China\n",
        "\n",
        "- Obsevation before: 31439\n",
        "- Obseervation after: 31071\n",
        "\n",
        "Note that, we loose the industry 43, ie: electricity supply. "
      ],
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "id": "r728ZBMktXse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_credit_pol_china = pd.merge(credit_china,\n",
        "         df_pollution_ind2,\n",
        "         left_on= 'CIC',\n",
        "         right_on = 'ind2',\n",
        "         how = 'inner'\n",
        "        )\n",
        "df_credit_pol_china['ind2'].sort_values().unique()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "T9VME1mewBKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_ts_pollution(l_df = [df_pollution,\n",
        "                          df_pollution_ind2,\n",
        "                          df_credit_pol_china],\n",
        "                  step = 3,\n",
        "                  move_to_drive  = False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "yX3jmJ6O_tyl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add FYP data\n",
        "\n",
        "We merge the provincial objective and keeps only the variables of interest which is the total target:\n",
        "\n",
        "- Target_Reduction_SO2:  How many tons of SO2 a province has to reduce\n",
        "\n",
        "- Obsevation before: 31071\n",
        "- Obseervation after: 31071"
      ],
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "id": "NrHviNiL8GBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_final_credit = pd.merge(df_credit_pol_china,\n",
        "                           fyp,\n",
        "                           how = 'inner',\n",
        "                           left_on = 'prov2013',\n",
        "                           right_on = 'province_ch'\n",
        "        )\n",
        "df_final_credit = df_final_credit.drop(columns = [\n",
        "    'ind2',\n",
        "    'Province',\n",
        "    'target_eletricity_sector',\n",
        "'province_ch'])\n",
        "df_final_credit = df_final_credit.rename(index=str,\n",
        "                                         columns={\"Total_targeted\":\n",
        "                                                  \"Total_targeted_SO2\"})\n",
        "df_final_credit.sort_values(by = ['year', 'geocode4_corr', 'CIC']).head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "cZYiPQMDOd5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_final_credit.shape"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "jdkETG9jOEpI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3 Create alternative variables\n",
        "\n",
        "The next step is crucial for our analysis. We need to construct the SO2 reduction mandate at the city level. We follow the paper of Chen, The consequences of spatially differentiated water pollution regulation in China. \n",
        "\n",
        "Another key element of the paper is the capacity of a firm to relocate following a stringent environmental policy. We need to construct a second variable to capture this effect\n",
        "\n",
        "### Compute city mandate\n",
        "\n",
        "The formula used in Chen paper to compute the COD mandate is the following\n",
        "\n",
        "$$\\Delta C O D_{c, 05-10}=\\Delta C O D_{p, 05-10} \\times \\sum_{i=1}^{27} \\mu_{i} \\frac{\\text { output value of industry } i \\text { in city } c}{\\text { output value of industry } i \\text { in province } p}$$\n",
        "\n",
        "In our case, we slightly change the notation since we care about SO2\n",
        "\n",
        "$$\\Delta SO2_{c, 05-10}=\\Delta SO2_{p, 05-10} \\times \\sum_{i=1}^{27} \\mu_{i} \\frac{\\text { output value of industry } i \\text { in city } c}{\\text { output value of industry } i \\text { in province } p}$$\n",
        "\n",
        "The target are not in the same scale as the original data\n",
        "\n",
        "- Total_targeted_SO2:  10.000 tons\n",
        "- tso2: Kilo\n",
        "- ttoutput: 万元\n",
        "\n",
        "- cod（千克）\n",
        "- industrial soot（千克）\n",
        "- smoke and dust（千克） \n",
        "-so2（千克）wasted gas（万标立方米）\n",
        "- wasted water（吨）\n",
        "\n",
        "It does not matter much since rescaling the dependant variable (`tso2`) does not affect the coefficient\n",
        "\n",
        "|  Stat  | SO2_05_city_reconstructed    | \ttso2_mandate_c\t   | SO2_obj_2010\t   | SO2_perc_reduction_c    |\n",
        "| ------ | ---------------------------- | ------------------ | --------------- | ----------------------- |\n",
        "| count\t | 285.000000\t                  | 285.000000\t        | 285.000000      | \t285.000000             |\n",
        "| mean\t  | 0.888336\t                    | 0.105859\t          | 0.782477\t       | 0.010455                |\n",
        "| std\t   | 1.013916\t                    | 0.156379\t          | 0.882085\t       | 0.022353                |\n",
        "| min\t   | 0.017006\t                    | 0.000000\t          | 0.016671\t       | 0.000000                |\n",
        "| 25%\t   | 0.246516\t                    | 0.019506\t          | 0.231384\t       | 0.001947                |\n",
        "| 50%\t   | 0.555758\t                    | 0.053844\t          | 0.490765\t       | 0.005210                |\n",
        "| 75%\t   | 1.166767\t                    | 0.122758\t          | 1.018420\t       | 0.011027                |\n",
        "| max\t   | 8.370000\t                    | 1.327908\t          | 7.370000        | \t0.258852               |\n",
        "\n",
        "\n",
        "The data can be found in the [spreadsheet China_cities_target_so2.csv](https://docs.google.com/spreadsheets/d/1z3A_I8_StdyNL5O38s2l9hx6W3VR49CGVmaosypjFMA/edit#gid=550420287)"
      ],
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "id": "B5PLFF_3Qh6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_final_credit['SO2_emissions_2005'] = df_final_credit['SO2_emissions_2005'].astype('float')\n",
        "df_final_credit['Total_targeted_SO2'] = df_final_credit['Total_targeted_SO2'].astype('float')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "KahASfKzbYQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_final_credit['ttoutput'].quantile(.95)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "QHfqxfr47pPU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The city reduction in percentage is equal to the province percentage"
      ],
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "id": "6eyU16j03zqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def city_mandate(df, var,\n",
        "                 # output,\n",
        "                 move_to_drive=False):\n",
        "    \"\"\" \n",
        "    This function compute:\n",
        "\n",
        "    - SO2 in 2005: reconstructed from the SO2 objective in 2010 + the reduction mandate\n",
        "    - SO2 mandate: reconstructed from the formula in Chen paper \n",
        "    - SO2 2010 objective: city reduction  * province objective\n",
        "    - SO2 reduction: reconstructed with city mandate / SO2 province in 2005. In this sense, the\n",
        "    percentage change by city\n",
        "    \"\"\"\n",
        "\n",
        "    df_temp = df.copy()\n",
        "    df_temp = df_temp[df_temp['year'] == 2005]\n",
        "\n",
        "    if var == 'tso2':\n",
        "        obj = 'Total_targeted_SO2'\n",
        "        val_2005 = 'SO2_emissions_2005'\n",
        "\n",
        "        df_temp[obj] = df_temp[obj].astype('float')\n",
        "        df_temp[val_2005] = df_temp[val_2005].astype('float')\n",
        "\n",
        "    else:\n",
        "        obj = 'Total_targeted_COD'\n",
        "        #val_2005 = 'SO2_emissions_2005'\n",
        "        df_temp[obj] = df_temp[obj].astype('float')\n",
        "\n",
        "    df_temp[var] = df_temp[var]\n",
        "\n",
        "    # Get the objective for each province\n",
        "    cod_so2_target = df_temp.groupby(['prov2013']).agg({obj: 'min',\n",
        "                                                        val_2005: 'min'\n",
        "                                                        }).reset_index()\n",
        "\n",
        "    # Compute the reduction mandate, ie how much to reduce by province\n",
        "    cod_so2_target['reduction'] = cod_so2_target[val_2005] - \\\n",
        "        cod_so2_target[obj]\n",
        "\n",
        "    # Compute the total SO2 by industry\n",
        "    polution_isic = df_temp.groupby(['CIC']).agg({var: 'sum'})\n",
        "\n",
        "    # Compute the percentage CO2 by industry over the total\n",
        "    polution_isic_perc = polution_isic.apply(\n",
        "        lambda x: x / float(x.sum())).reset_index()\n",
        "\n",
        "    #### Compute the total output by city and industry. Need to keep\n",
        "    #### Province to merge with objective\n",
        "    city_isic_output = df_temp[df_temp['year'] == 2005].groupby(\n",
        "        ['prov2013',\n",
        "         'citycn',\n",
        "         'CIC']).agg({\n",
        "        'ttoutput': 'sum'}).reset_index()\n",
        "\n",
        "    ##### Compute the total output by province\n",
        "    province_isic_output = df_final_credit[\n",
        "        df_final_credit['year'] == 2005].groupby(['prov2013', 'CIC']).agg(\n",
        "        {'ttoutput': 'sum'}).reset_index()\n",
        "\n",
        "    #### Merge the total output by city and total output by province\n",
        "    df_poll = pd.merge(city_isic_output, province_isic_output,\n",
        "                       how='left', on=['prov2013', 'CIC'],\n",
        "                       suffixes=('_city', '_province'))\n",
        "\n",
        "    #### Merge pollution with output\n",
        "    df_poll = pd.merge(df_poll, polution_isic_perc, how='left', on='CIC')\n",
        "\n",
        "    #### Compute the RHS of the formula,\n",
        "    #### ie ratio output city/province * alpha (intensity SO2)\n",
        "    name_var = 'ratio_o_' + var\n",
        "    df_poll[name_var] = df_poll[var] * df_poll['ttoutput_city'] / \\\n",
        "        df_poll['ttoutput_province']\n",
        "\n",
        "    #### Sum over each city\n",
        "    df_ratio_city = df_poll.groupby(['prov2013', 'citycn']).agg({\n",
        "        name_var: 'sum'}).reset_index()\n",
        "\n",
        "    ### Merge with objective\n",
        "    df_ratio_city_ = pd.merge(df_ratio_city, cod_so2_target,\n",
        "                              how='left',\n",
        "                              on='prov2013')\n",
        "\n",
        "    ### Compute the reduction mandate at the city level\n",
        "    name_mandate = var + '_mandate_c'\n",
        "    df_ratio_city_[name_mandate] = (df_ratio_city_[name_var] *\n",
        "                                    df_ratio_city_['reduction'])/10\n",
        "    # SO2_obj_2010 is the new amount of SO2 to reach\n",
        "    df_ratio_city_['SO2_obj_2010'] = (df_ratio_city_[name_var] *\n",
        "                                      df_ratio_city_[obj])/10\n",
        "\n",
        "    df_ratio_city_['SO2_c_2005'] = (df_ratio_city_[name_var] *\n",
        "                                    df_ratio_city_[val_2005])/10\n",
        "\n",
        "    #name_perc = 'perc_target_' + var\n",
        "    # df_ratio_city_[name_perc] = (df_ratio_city_['SO2_c_2005'] - \\\n",
        "    #                                 df_ratio_city_['temp'])/ \\\n",
        "    # df_ratio_city_['SO2_c_2005']\n",
        "\n",
        "    # Add SO2 reconstructed\n",
        "    df_ratio_city_['SO2_05_city_reconstructed'] = df_ratio_city_[\n",
        "        'SO2_obj_2010'] + df_ratio_city_['tso2_mandate_c']\n",
        "\n",
        "    df_ratio_city_ = df_ratio_city_[['citycn', name_mandate, 'SO2_obj_2010',\n",
        "                                     'SO2_05_city_reconstructed',\n",
        "                                     ]]\n",
        "\n",
        "    # Add percentage change\n",
        "    cityprov = df[['prov2013', 'citycn',\n",
        "                   'SO2_emissions_2005']].drop_duplicates()\n",
        "    df_ratio_city_ = pd.merge(\n",
        "        df_ratio_city_, cityprov, how='left', on='citycn')\n",
        "\n",
        "    df_ratio_city_['SO2_perc_reduction_c'] = (\n",
        "        df_ratio_city_['tso2_mandate_c'] / df_ratio_city_['SO2_emissions_2005']) * 10\n",
        "\n",
        "    df_ratio_city_ = df_ratio_city_[['citycn', 'prov2013', 'SO2_05_city_reconstructed',\n",
        "                                     name_mandate, 'SO2_obj_2010',\n",
        "                                     'SO2_perc_reduction_c'\n",
        "                                     ]]\n",
        "\n",
        "    #df_ratio_city_ = pd.merge(df_ratio_city_, output, how='inner', on='citycn')\n",
        "    # df_ratio_city_['intensity_mandate'] = df_ratio_city_[\n",
        "    #    'tso2_mandate_c'] / (df_ratio_city_['ttoutput']/1000000)\n",
        "\n",
        "    #df_ratio_city_ = df_ratio_city_.drop(columns = 'ttoutput')\n",
        "\n",
        "    df_output = {\n",
        "\n",
        "        'polution_isic_perc': polution_isic_perc,\n",
        "        'df_poll': df_poll,\n",
        "        'df_ratio_city': df_ratio_city_,\n",
        "        'name_mandate': name_mandate\n",
        "    }\n",
        "\n",
        "    if move_to_drive:\n",
        "        # https://docs.google.com/spreadsheets/d/1z3A_I8_StdyNL5O38s2l9hx6W3VR49CGVmaosypjFMA/edit\n",
        "        # Google Sheet name: temp_China_cities_target_so2.csv\n",
        "        mime_type = \"application/vnd.google-apps.spreadsheet\"\n",
        "        df_output['df_ratio_city'].to_csv(\n",
        "            'China_cities_target_so2.csv',\n",
        "            sep=',',\n",
        "            header=True,\n",
        "            index=False,\n",
        "            chunksize=100000,\n",
        "            encoding='utf-8')\n",
        "        cdr.upload_file_root(mime_type, 'China_cities_target_so2.csv')\n",
        "        cdr.move_file(file_name='China_cities_target_so2.csv',\n",
        "                      folder_name='Pollution_china')\n",
        "        os.remove('China_cities_target_so2.csv')\n",
        "\n",
        "    return df_output"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "code_folding": [],
        "colab": {},
        "colab_type": "code",
        "id": "psNFrmuYqT_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "city_reduction = city_mandate(df = df_final_credit,\n",
        "                         var = 'tso2',\n",
        "                         #output = output_2005,\n",
        "                         move_to_drive = False)['df_ratio_city'].sort_values(\n",
        "    by = 'tso2_mandate_c',\n",
        "ascending = False)\n",
        "city_reduction.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "iZfhtX5IsEKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "city_reduction.describe()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "JSQP0-xBdcNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indu_name = df_final_credit[['CIC', 'Industry Name']].drop_duplicates()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "-BxsiumpzZ9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "city_cic_coef = city_mandate(\n",
        "    df=df_final_credit,\n",
        "    var='tso2',\n",
        "    #output = output_2005,\n",
        "    move_to_drive=False)['polution_isic_perc'].merge(indu_name).rename(\n",
        "        index=str, columns={'tso2': 'industry_so2_share'})\n",
        "\n",
        "city_cic_coef = city_cic_coef[['CIC',\n",
        "                               'Industry Name',\n",
        "                               'industry_so2_share']]\n",
        "\n",
        "sheetid = '1z3A_I8_StdyNL5O38s2l9hx6W3VR49CGVmaosypjFMA'\n",
        "sheetname = 'industry_weights'\n",
        "\n",
        "range_data = 'A1:C30'\n",
        "#cdr.add_data_to_spreadsheet(data=city_cic_coef.to_numpy().tolist(),\n",
        "#                            sheetID=sheetid,\n",
        "#                            sheetName=sheetname,\n",
        "#                            rangeData=range_data,\n",
        "#                            headers=list(city_cic_coef))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "i7QB6ho3zZ9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sheetid = '1z3A_I8_StdyNL5O38s2l9hx6W3VR49CGVmaosypjFMA'\n",
        "sheetname = 'industry_weights'\n",
        "\n",
        "range_data = 'A1:B30'\n",
        "#cdr.add_data_to_spreadsheet(data=city_cic_coef.to_numpy().tolist(),\n",
        "#                            sheetID=sheetid,\n",
        "#                            sheetName=sheetname,\n",
        "#                            rangeData=range_data,\n",
        "#                            headers=list(city_cic_coef))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "LSAuw9hizZ9p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test\n",
        "\n",
        "In this section we perform some test to make sure the computation is right. For instance, we should roughly get the same values of tso2_mandate_c and SO2_obj_2010 for Shanghai, Tianjin, Beijing and Chongqing.\n",
        "\n",
        "\n",
        "Besides, the sum of each city should match the province level\n",
        "\n",
        "The first test aims at computing the different between the true SO2 emission by province and the reconstructed one. If the computation are corrects, the difference should be around 0\n",
        "\n",
        "|  Stat | \tSO2_05_city_reconstructed    | \tSO2_emissions_2005   | \tdif          |\n",
        "| ----- | ----------------------------- | --------------------- | ------------- |\n",
        "| count | \t30.000000\t                   | 30.000000             | \t3.000000e+01 |\n",
        "| mean\t | 8.439193\t                     | 8.497333\t             | 5.814010e-02  |\n",
        "| std\t  | 5.108585\t                     | 5.101497\t             | 1.083516e-01  |\n",
        "| min\t  | 0.139286\t                     | 0.220000\t             | -1.776357e-15 |\n",
        "| 25%\t  | 4.904120\t                     | 5.092500\t             | 2.220446e-16  |\n",
        "| 50%\t  | 7.768635\t                     | 7.770000\t             | 4.240052e-03  |\n",
        "| 75%\t  | 12.977500\t                    | 12.977500             | \t4.095134e-02 |\n",
        "| max\t  | 20.030000\t                    | 20.030000\t            | 3.481846e-01  |\n",
        "\n",
        "The second test compute the difference between the effective target in 2010 SO2 at the province level\n",
        "and the reconstructed SO2 computed as the sum of SO2 by city, at the privince level\n",
        "    \n",
        " The difference should be around 0\n",
        " \n",
        " |  Stat  | \t\treconstructed_obj_SO2\t   | Total_targeted_SO2   | \tdif      |\n",
        "| ------ | -------------------------- | -------------------- | --------- |\n",
        "| count\t | 30.000000\t                 | 30.000000\t           | 30.000000 |\n",
        "| mean\t  | 8.212868\t                  | 7.488333\t            | -0.724535 |\n",
        "| std\t   | 5.061923\t                  | 4.267000\t            | 0.930495  |\n",
        "| min\t   | 0.139286\t                  | 0.220000\t            | -3.666304 |\n",
        "| 25%\t   | 4.619681\t                  | 4.425000\t            | -1.231877 |\n",
        "| 50%\t   | 7.229558\t                  | 6.960000\t            | -0.337300 |\n",
        "| 75%\t   | 12.659837\t                 | 11.195000\t           | -0.001951 |\n",
        "| max\t   | 19.686304\t                 | 16.020000\t           | 0.340755  |"
      ],
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "id": "a4mSiOsMfQ5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_SO2_prov_2005(df):\n",
        "    \n",
        "    \"\"\"\n",
        "    Compute the difference between the effective SO2 at the province level\n",
        "    and the reconstructed SO2 computed as the sum of SO2 by city, at the privince level\n",
        "    \n",
        "    The difference should be around 0\n",
        "    \"\"\"\n",
        "\n",
        "    aggregated_df = df.groupby('prov2013').agg({\n",
        "        'SO2_05_city_reconstructed': 'sum'\n",
        "    })\n",
        "    aggregated_df = aggregated_df.reset_index()\n",
        "\n",
        "    cityprov = df_final_credit[['prov2013',\n",
        "                                'SO2_emissions_2005']].drop_duplicates()\n",
        "    aggregated_df = pd.merge(aggregated_df, cityprov,\n",
        "                             how='left', on='prov2013')\n",
        "    aggregated_df['SO2_emissions_2005'] = aggregated_df['SO2_emissions_2005']/10\n",
        "\n",
        "    aggregated_df['dif'] = aggregated_df['SO2_emissions_2005'] - \\\n",
        "        aggregated_df['SO2_05_city_reconstructed']    \n",
        "    \n",
        "    return aggregated_df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "AJdyhAYf1-sY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_SO2_prov_2005(city_reduction).describe()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "XNJtNGq21-sa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check_SO2_prov_2005(city_reduction)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "EuSZWYoSzZ9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validation(so2_05, reduction):\n",
        "    \n",
        "    val = np.sum(so2_05 * (1 - reduction))\n",
        "    return val"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "GlOkbLc71-sc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_SO2_obj(df):\n",
        "    \"\"\"\n",
        "    Compute the difference between the effective target in 2010 SO2 at the province level\n",
        "    and the reconstructed SO2 computed as the sum of SO2 by city, at the privince level\n",
        "    \n",
        "    The difference should be around 0\n",
        "    \"\"\"\n",
        "\n",
        "    cityprov = df_final_credit[['prov2013',\n",
        "                                'Total_targeted_SO2']].drop_duplicates()\n",
        "\n",
        "    city_val = df.groupby('prov2013').apply(\n",
        "      lambda x: validation(so2_05 = x['SO2_05_city_reconstructed'],\n",
        "                           reduction = x['SO2_perc_reduction_c']\n",
        "                                  )).reset_index()\n",
        "    city_val.columns = ['prov2013', 'reconstructed_obj_SO2']\n",
        "    city_val = pd.merge(city_val, cityprov, how = 'left', on = 'prov2013')\n",
        "    city_val['Total_targeted_SO2'] = city_val['Total_targeted_SO2'] / 10\n",
        "    city_val['dif'] = city_val['Total_targeted_SO2'] - city_val['reconstructed_obj_SO2']\n",
        "    return city_val"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "QhYUYRLH1-sh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_SO2_obj(df = city_reduction)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "Gnhk1OE81-sj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute spatial relocation\n",
        "\n",
        "The next variable of interest is the spatial relocation. The spatial relocation is computed with different metrics. However, for the final dataset, we retain only one candidate, the city effort over the output. \n",
        "\n",
        "\n",
        "- `avg_ij_o_city_mandate`: reduction mandate city i over output city i  - average reduction cities j over output city  cities j\n",
        "\n",
        "We will compute the following\n",
        "\n",
        "**Reduction mandate**\n",
        "\n",
        "- `avg_ij_city_mandate`: reduction mandate city i - average reduction cities j\n",
        "- `w_avg_ij_city_mandate`: reduction mandate city i - average reduction cities j, adjusted by the distance\n",
        "\n",
        "- `avg_ij_o_city_mandate`: reduction mandate city i - average reduction cities j over output city i - average citiesj\n",
        "- `w_avg_ij_o_city_mandate`: reduction mandate city i - average reduction cities j over output city i - average citiesj, adjusted by the distance\n",
        "- `d_avg_ij_o_city_mandate`: Dummy that takes the value of 1 if avg_ij_city_mandate > 0.0005554856\n",
        "- `d_w_avg_ij_o_city_mandate`:Dummy that takes the value of 1 if w_avg_ij_o_city_mandate > 0.000532341\n",
        "\n",
        "**Percentage reduction**\n",
        "\n",
        "- `avg_ij_perc_city_mandate`: Percentage reduction mandate city i - average reduction cities j\n",
        "- `w_avg_ij_perc_city_mandate`: Percentage reduction mandate city i - average reduction cities j, adjusted by the distance\n",
        "- `avg_ij_o_perc_city_mandate`: Percentage reduction mandate city i - average reduction cities j over output city i - average citiesj\n",
        "- `w_avg_ij_o_perc_city_mandate`: Percentage reduction mandate city i - average reduction cities j over output city i - average citiesj, adjusted by the distance\n",
        "\n",
        "|  Stat  |  \tavg_ij_o_city_mandate     |\n",
        "| ------ | --------------------------- |\n",
        "| count\t | 285.000000                  |\n",
        "| mean\t  | 0.001397                    |\n",
        "| std\t   | 0.067669                    |\n",
        "| min\t   | -0.675803                   |\n",
        "| 25%\t   | 0.000134                    |\n",
        "| 50%\t   | 0.003311                    |\n",
        "| 75%\t   | 0.006748                    |\n",
        "| max\t   | 0.412381                    |\n",
        "\n",
        "The data can be found in the [spreadsheet China_cities_target_so2.csv](https://docs.google.com/spreadsheets/d/1z3A_I8_StdyNL5O38s2l9hx6W3VR49CGVmaosypjFMA/edit#gid=550420287)\n",
        "\n",
        "\n",
        "#### haversine Distance\n",
        "\n",
        "The distances have been computed using this [Notebook](https://drive.google.com/open?id=1Bp60rL-QjL6_tIB6yLwVvHMXti2mZAsJ):\n",
        "\n",
        "and this formula:\n",
        "\n",
        "This uses the ‘haversine’ formula to calculate the great-circle distance between two points – that is, the shortest distance over the earth’s surface – giving an ‘as-the-crow-flies’ distance between the points\n",
        "\n",
        "$$\\begin{aligned} \\text { Haversine } & \\mathrm{a}=\\sin ^{2}(\\Delta \\varphi / 2)+\\cos \\varphi_{1} \\cdot \\cos \\varphi_{2} \\cdot \\sin ^{2}(\\Delta \\lambda / 2) \\\\ \\text { formula: } & \\mathrm{c}=2 \\cdot \\operatorname{atan} 2(\\sqrt{\\mathrm{a}}, \\sqrt{(1-\\mathrm{a})}) \\\\ & \\mathrm{d}=\\mathrm{R} \\cdot \\mathrm{c} \\end{aligned}$$\n",
        "\n",
        "$\\begin{array}{c}{\\text { where } \\varphi \\text { is latitude, } \\lambda \\text { is longitude, } \\mathrm{R} \\text { is earth's radius (mean radius =6, 371km); }} \\\\ {\\text { note that angles need to be in radians to pass to trig functions! }}\\end{array}$\n",
        "\n",
        "|  Stat       |  Distance    |\n",
        "| ----------- | ------------ |\n",
        "| count       | 80940.000000 |\n",
        "| mean        |  1310.288115 |\n",
        "| std         | 1020.579905  |\n",
        "| min         |    0.000000  |\n",
        "| 25%         |  714.055908  |\n",
        "| 50%         | 1143.914211  |\n",
        "| 75%         | 1635.457978  |\n",
        "| max         | 9288.892591  |"
      ],
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "id": "RyLaG_szqPGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_col = ['Cities_1','Cities_2','latitude_Cities1','longitude_Cities1',\n",
        "            'latitude_Cities2', 'longitude_Cities2','distance']\n",
        "\n",
        "sheetid = '1nFSg83a0P-XMXEgP58HD0RIZ-qV3ZafOwZ1jqi6EFHk'\n",
        "range_name = 'China_cities_distance.csv!A2:G80942'\n",
        "\n",
        "distance = service['sheet'].spreadsheets().values().get(\n",
        "    spreadsheetId= sheetid,\n",
        "    range=range_name).execute()\n",
        "distance = pd.DataFrame(distance.get('values', []),\n",
        "                              columns=list_col)\n",
        "distance['distance'] = distance['distance'].astype('float')\n",
        "distance['distance'].describe()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "YzKYQo6hqTWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(x):\n",
        "  \n",
        "  norm = (x - np.min(x)) / (np.max(x) - np.min(x))\n",
        "  \n",
        "  return norm"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "MtG_kXgMKWw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reallocation(city_mandate, city_distance, output, print_plot= False,\n",
        "                 move_to_drive=False):\n",
        "    \"\"\"\n",
        "    Comptue the reallocation\n",
        "    \"\"\"\n",
        "\n",
        "    list_drop = ['citycn', 'prov2013',\n",
        "                 'SO2_05_city_reconstructed', 'SO2_obj_2010']\n",
        "\n",
        "    # Merge output with distance\n",
        "    distance = pd.merge(city_distance, output_2005, how='inner',\n",
        "                        left_on='Cities_2', right_on='citycn').drop(columns='citycn')\n",
        "    distance = pd.merge(distance, output_2005, how='inner',\n",
        "                        left_on='Cities_1', right_on='citycn',\n",
        "                       suffixes=('_j', '_i')).drop(columns='citycn')\n",
        "    \n",
        "    df_temp_mandate = pd.merge(distance[['Cities_1', 'Cities_2', 'distance',\n",
        "                                       'ttoutput_i', 'ttoutput_j']],\n",
        "                               city_mandate, how=\"left\",\n",
        "                               left_on='Cities_1',\n",
        "                               right_on='citycn').drop(columns=list_drop)\n",
        "\n",
        "    #Merge with mandate\n",
        "\n",
        "    df_dist_mandate=pd.merge(df_temp_mandate,\n",
        "                               city_mandate,\n",
        "                               how=\"left\",\n",
        "                               left_on='Cities_2',\n",
        "                               right_on='citycn',\n",
        "                               suffixes=('_i', '_j')).drop(columns=list_drop)\n",
        "\n",
        "    df_dist_mandate=df_dist_mandate[df_dist_mandate['distance'] != 0]\n",
        "    \n",
        "    #### Compute all the variables\n",
        "    df_vars=construct_vars_reac(df_dist_mandate)\n",
        "    \n",
        "    dic_df={\n",
        "        'bi_cities': df_dist_mandate,\n",
        "        'reallocation': df_vars\n",
        "    }\n",
        "\n",
        "    # Create plots\n",
        "    if print_plot:\n",
        "        f, axes=plt.subplots(4, 2, figsize=(14, 14), sharex=False)\n",
        "\n",
        "        for x, name in enumerate([['tso2_mandate_c_j', 'w_avg_ij_o_city_mandate',\n",
        "                               'avg_ij_o_city_mandate'],\n",
        "                              ['SO2_perc_reduction_c_j', 'avg_ij_perc_city_mandate',\n",
        "                               'avg_ij_o_perc_city_mandate']]):\n",
        "            for y, g in enumerate(name):\n",
        "                sns_plot=sns.distplot(df_vars[g],\n",
        "                                    color=\"b\", ax=axes[y, x])\n",
        "\n",
        "        for x, x_name in enumerate(['d_avg_ij_o_city_mandate', 'd_w_avg_ij_o_city_mandate']):\n",
        "            sns_plot=sns.countplot(x=x_name, data=df_vars, ax=axes[3, x])\n",
        "\n",
        "    if move_to_drive:\n",
        "\n",
        "        # Spreadsheet:\n",
        "        # https://docs.google.com/spreadsheets/d/1z3A_I8_StdyNL5O38s2l9hx6W3VR49CGVmaosypjFMA/edit\n",
        "        # name China_cities_target_so2.csv\n",
        "        l_df_final=df_vars.to_numpy().tolist()\n",
        "        headers=list(df_vars)\n",
        "        rangeData=\"A1:Z287\"\n",
        "\n",
        "    # Name spreadsheet temp_China_cities_target_so2.csv\n",
        "        cdr.add_data_to_spreadsheet(data=l_df_final,\n",
        "                                    sheetID='1z3A_I8_StdyNL5O38s2l9hx6W3VR49CGVmaosypjFMA',\n",
        "                                    sheetName='reallocation',\n",
        "                                    rangeData=rangeData,\n",
        "                                    headers=headers)\n",
        "\n",
        "    return dic_df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "-G__WTZO1-tI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_vars_reac(df):\n",
        "    \"\"\"\n",
        "    compute:\n",
        "    avg_ij = 1/n [city_i - city_j]\n",
        "    weigthed_if = 1/n [(city_i - city_j)/distance_ij]\n",
        "    Normalize\n",
        "    dummy\n",
        "    \"\"\"\n",
        "\n",
        "    df_intermediary = df.copy()\n",
        "\n",
        "    # Compute the metrics in absolte value and in percentage change\n",
        "    for x in ['city_mandate', 'perc_city_mandate']:\n",
        "        name = [\n",
        "            'avg_ij_o_' + x, 'w_avg_ij_o_' + x, 'avg_ij_' + x, 'w_avg_ij_' + x,\n",
        "            'avg_io_jo_' + x, 'w_avg_io_jo_' + x, 'abv_avg_io_jo_' + x,\n",
        "            'abv_w_avg_io_jo_' + x\n",
        "        ]\n",
        "\n",
        "        if x == 'city_mandate':\n",
        "\n",
        "            df_intermediary[name[0]] = (\n",
        "                df['tso2_mandate_c_i'] - df['tso2_mandate_c_j']) / (\n",
        "                    (df['ttoutput_i'] - df['ttoutput_j']) / 100000)\n",
        "\n",
        "            df_intermediary[\n",
        "                name[1]] = df_intermediary[name[0]] / (df['distance'] / 1000)\n",
        "\n",
        "            df_intermediary[name[2]] = (\n",
        "                df['tso2_mandate_c_i'] - df['tso2_mandate_c_j'])\n",
        "\n",
        "            df_intermediary[\n",
        "                name[3]] = df_intermediary[name[2]] / (df['distance'] / 1000)\n",
        "\n",
        "            df_intermediary[name[4]] = df['tso2_mandate_c_i'] / \\\n",
        "                (df['ttoutput_i'] / 100000)\n",
        "\n",
        "            df_intermediary[name[5]] = df['tso2_mandate_c_j'] / \\\n",
        "                (df['ttoutput_j'] / 100000)\n",
        "\n",
        "            df_intermediary[name[6]] = np.where(\n",
        "                df['tso2_mandate_c_i'] > df['tso2_mandate_c_j'],\n",
        "                df_intermediary[name[4]], 0)\n",
        "\n",
        "            df_intermediary[name[7]] = np.where(\n",
        "                df['tso2_mandate_c_i'] > df['tso2_mandate_c_j'],\n",
        "                df_intermediary[name[5]], 0)\n",
        "\n",
        "        else:\n",
        "\n",
        "            df_intermediary[name[0]] = (df['SO2_perc_reduction_c_i'] -\n",
        "                                        df['SO2_perc_reduction_c_j']) / \\\n",
        "                ((df['ttoutput_i'] - df['ttoutput_j']) / 100000)\n",
        "\n",
        "            df_intermediary[\n",
        "                name[1]] = df_intermediary[name[0]] / (df['distance'] / 1000)\n",
        "            df_intermediary[name[2]] = (\n",
        "                df['SO2_perc_reduction_c_i'] - df['SO2_perc_reduction_c_j'])\n",
        "\n",
        "            df_intermediary[\n",
        "                name[3]] = df_intermediary[name[2]] / (df['distance'] / 1000)\n",
        "\n",
        "    df_intermediary = df_intermediary.drop(columns=[  # 'distance',\n",
        "        'SO2_perc_reduction_c_i'  # , 'tso2_mandate_c_i'\n",
        "    ])\n",
        "\n",
        "    # return df_intermediary\n",
        "    # df_intermediary = df_intermediary.drop(columns = 'ttoutput_i', 'ttouput_j')\n",
        "    df_intermediary = df_intermediary.groupby('Cities_1').mean()\n",
        "    \n",
        "# df_intermediary['tso2_mandate_c_i'] - \\\n",
        "    df_intermediary['avg_ij_o_city_mandate'] = (df_intermediary['tso2_mandate_c_i'] /\n",
        "                               (df_intermediary['ttoutput_i']/100000)) - (df_intermediary['tso2_mandate_c_j'] /\n",
        "                               (df_intermediary['ttoutput_j']/100000)) \n",
        "\n",
        "    # df_intermediary['dist_tso2_mandate_c_j'] = df_intermediary['tso2_mandate_c_j'] / \\\n",
        "    #    (df_intermediary['distance']/1000)\n",
        "    # df_intermediary['dist_SO2_perc_reduction_c_j'] = df_intermediary['SO2_perc_reduction_c_j'] / \\\n",
        "    #    (df_intermediary['distance']/1000)\n",
        "\n",
        "    # for x in ['avg_ij_o_city_mandate', 'w_avg_ij_o_city_mandate']:\n",
        "    #name = 'd_' + x\n",
        "    #name_n = 'norm_' + x\n",
        "    # if x ==\n",
        "\n",
        "    #df_intermediary['avg_io_jo_city_mandate'] = df_intermediary['avg_io_jo_city_mandate'] - \\\n",
        "    #    df_intermediary['w_avg_io_jo_city_mandate']\n",
        "    #df_intermediary['w_avg_io_jo_city_mandate'] = df_intermediary['avg_io_jo_city_mandate'] / \\\n",
        "    #    df_intermediary['distance']\n",
        "\n",
        "    #df_intermediary['abv_avg_io_jo_city_mandate'] = df_intermediary['abv_avg_io_jo_city_mandate'] - \\\n",
        "    #    df_intermediary['abv_w_avg_io_jo_city_mandate']\n",
        "\n",
        "    #df_intermediary['abv_w_avg_io_jo_city_mandate'] = df_intermediary['abv_avg_io_jo_city_mandate'] / \\\n",
        "    #    df_intermediary['distance']\n",
        "\n",
        "    mean_avg_ij_o_city_mandate = np.mean(df_intermediary['avg_ij_o_city_mandate'])\n",
        "    wmean_avg_ij_o_city_mandate = np.mean(df_intermediary['w_avg_ij_o_city_mandate'])\n",
        "    \n",
        "    df_intermediary['d_avg_ij_o_city_mandate'] = np.where(\n",
        "        df_intermediary['avg_ij_o_city_mandate'] < mean_avg_ij_o_city_mandate, 'yes', 'no')\n",
        "\n",
        "    df_intermediary['d_w_avg_ij_o_city_mandate'] = np.where(\n",
        "        df_intermediary['w_avg_ij_o_city_mandate'] < wmean_avg_ij_o_city_mandate, 'yes', 'no')\n",
        "    #df_intermediary[name_n] = normalize(df_intermediary[x])\n",
        "\n",
        "    df_intermediary = df_intermediary.reset_index()\n",
        "    df_intermediary = df_intermediary.rename(\n",
        "        index=str, columns={\n",
        "            \"Cities_1\": 'citycn'\n",
        "        }).sort_values(\n",
        "            by='avg_ij_city_mandate', ascending=True)\n",
        "\n",
        "    return df_intermediary"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "nHe7Ob4k1-tr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_final_credit['year'] = df_final_credit['year'].astype('int')\n",
        "output_2005 = df_final_credit[df_final_credit['year'] == 2005].groupby('citycn')[\n",
        "    'ttoutput'].sum().reset_index()\n",
        "\n",
        "output_2005['ttoutput'].isna().sum()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "qMTolS9E1-tz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp = reallocation(city_mandate=city_reduction,\n",
        "             city_distance=distance,\n",
        "             output = output_2005,\n",
        "             print_plot = False,\n",
        "             move_to_drive=True)['reallocation']#.head()\n",
        "temp.head()\n",
        "#temp[#temp['Cities_1'] == '天水' |\n",
        "#    temp['Cities_1'] == '金昌' #|\n",
        "    #temp['Cities_1'] == '西宁'\n",
        "#    ]\n",
        "#天水 金昌 西宁"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "7bXTIRdJ1-t3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp.sort_values(by = 'avg_ij_o_city_mandate')[['citycn', 'avg_ij_o_city_mandate', 'd_avg_ij_o_city_mandate', 'tso2_mandate_c_i', 'tso2_mandate_c_j']].corr()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#temp.sort_values(by = 'avg_ij_o_city_mandate')[['citycn', 'avg_ij_o_city_mandate', 'd_avg_ij_o_city_mandate', 'tso2_mandate_c_i', 'tso2_mandate_c_j']]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reallocation(city_mandate=city_reduction,\n",
        "             city_distance=distance,\n",
        "             output = output_2005,\n",
        "             print_plot = False,\n",
        "             move_to_drive=False)['reallocation'][['avg_ij_o_city_mandate']].describe()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "aH1rcvtl1-t5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute city-sector variables\n",
        "\n",
        "Last but not least, we need to compute the total fixed asset and employement by city and sector to make sure our model do not omit this crucial variable. We follow the litterature on credit constraint. \n",
        "\n",
        "To compute these two variables, we need to import the ASIF dataset from Big Query"
      ],
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "id": "1kKRjrL_FmM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "service_account = cs.get_storage_client()\n",
        "from GoogleDrivePy.google_platform import connect_cloud_platform\n",
        "\n",
        "ccp = connect_cloud_platform.connect_console(service_account = service_account,\n",
        "                                             colab = False)\n",
        "\n",
        "query = (\n",
        "  \"SELECT * \"\n",
        "    \"FROM China.df_ASIF_credit \"\n",
        "\n",
        ")\n",
        "\n",
        "df_ASIF_raw = service_account['bigquery_account'].query(\n",
        "    query,\n",
        "    # Location must match that of the dataset(s) referenced in the query.\n",
        "    location=\"US\",\n",
        ").to_dataframe()\n",
        "df_ASIF_raw['CIC'] = df_ASIF_raw['cic'].str.slice(stop=2)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "JvxCIuD8Fq9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def keepcolumn(df, method = 'value_added'):\n",
        "  \n",
        "  \"\"\"\n",
        "  Keep columns and remove negative values\n",
        "  - value_added\n",
        "  - sales\n",
        "  - output\n",
        "  \"\"\"\n",
        "\n",
        "  sum_va = df.groupby(['year', 'dq','CIC'])[['addval',\n",
        "                                             'sales',\n",
        "                                             'output',\n",
        "                                             'netfixed'\n",
        "                                              ]].sum().reset_index()\n",
        "\n",
        "  sum_va.columns = ['year', 'geocode4_corr','CIC',\n",
        "                    'value_added', 'sales',\n",
        "                    'output', 'fixed_asset']\n",
        "\n",
        "  sum_va = sum_va[(sum_va[method] > 0) & (sum_va['fixed_asset'] > 0)]\n",
        "  sum_va = sum_va[['year', 'geocode4_corr', 'CIC', method, 'fixed_asset']]\n",
        "  \n",
        "  return sum_va"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "WDXqM9O6GIm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keep = keepcolumn(df = df_ASIF_raw, method = 'value_added')\n",
        "keep.isna().sum()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "4lNOjsxLGofu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def employment(df):\n",
        "    \"\"\"\n",
        "    Compute the employemnt at the city sector year\n",
        "    \"\"\"\n",
        "\n",
        "    sum_cap_emp = df.groupby(['year', 'dq', 'CIC']).agg({\n",
        "        'employ': 'sum'}).reset_index()\n",
        "\n",
        "    # sum_cap_emp['ratio_cap_labor'] = sum_cap_emp['netfixed'] /\\\n",
        "    #                                 sum_cap_emp['employ']\n",
        "\n",
        "    sum_cap_emp.columns = ['year', 'geocode4_corr', 'CIC',\n",
        "                           'employment',\n",
        "                           # 'ratio_cap_labor'\n",
        "                           ]\n",
        "\n",
        "    sum_cap_emp = sum_cap_emp[['year', 'geocode4_corr',\n",
        "                               'CIC', 'employment']]\n",
        "\n",
        "    sum_cap_emp = sum_cap_emp.sort_values(by=['year', 'geocode4_corr',\n",
        "                                              'CIC'])\n",
        "    #sum_cap_emp = sum_cap_emp.replace([np.inf, -np.inf], np.nan)\n",
        "    sum_cap_emp = sum_cap_emp.dropna()\n",
        "    #sum_cap_emp = sum_cap_emp[sum_cap_emp['ratio_cap_labor'] > 0]\n",
        "    sum_cap_emp = sum_cap_emp[sum_cap_emp['employment'] > 0]\n",
        "    return sum_cap_emp"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "rC-fmKZRGslW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add provinces locations"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prov_loc = {\n",
        "    'prov2013': [\n",
        "        '广东省', '福建省', '浙江省', '上海市', '江苏省', '山东省', '河北省', '天津市', '海南省', '辽宁省',\n",
        "        '黑龙江省', '吉林省', '河南省', '安徽省', '山西省', '湖南省', '湖北省', '江西省', '广西壮族自治区',\n",
        "        '四川省', '贵州省', '云南省', '重庆市', '陕西省', '内蒙古自治区', '甘肃省', '宁夏回族自治区', '青海省',\n",
        "        '新疆维吾尔自治区', '北京市'\n",
        "    ],\n",
        "    'Provinces': [\n",
        "        'Guangdong', 'Fujian', 'Zhejiang', 'Shanghai', 'Jiangsu', 'Shandong',\n",
        "        'Hebei', 'Tianjin', 'Hainan', 'Liaoning', 'Heilongjiang', 'Jilin',\n",
        "        'Henan', 'Anhui', 'Shanxi', 'Hunan', 'Hubei', 'Jiangxi', 'Guangxi',\n",
        "        'Sichuan', 'Guizhou', 'Yunnan', 'Chongqing', 'Shannxi',\n",
        "        'Inner Mongolia', 'Gansu', 'Ningxia', 'Qinghai', 'Xinjiang', 'Beijing'\n",
        "    ],\n",
        "    'location': [\n",
        "        'Coastal', 'Coastal', 'Coastal', 'Coastal', 'Coastal', 'Coastal',\n",
        "        'Coastal', 'Coastal', 'Coastal', 'Northeast', 'Northeast', 'Northeast',\n",
        "        'Central', 'Central', 'Central', 'Central', 'Central', 'Central',\n",
        "        'Southwest', 'Southwest', 'Southwest', 'Southwest', 'Southwest',\n",
        "        'Northwest', 'Northwest', 'Northwest', 'Northwest', 'Northwest',\n",
        "        'Northwest', 'Coastal'\n",
        "    ]\n",
        "}\n",
        "\n",
        "df_prov_en = pd.DataFrame(prov_loc)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4:  Finalize dataset\n",
        "\n",
        "The last and most important step is to merge the different additional variable to the pollution data. \n",
        "\n",
        "We start with 31,071 observations and we end up with 25,404 observations and 20 variables\n",
        "\n",
        "The final dataset contains the following variables:\n",
        "\n",
        "|  Variable name               |  Description                                        |\n",
        "| ---------------------------- | --------------------------------------------------- |\n",
        "| year                         |   Year:  2002 - 2007                                |\n",
        "| Post                         |  Take value 1 if year > 2005                        |\n",
        "| prov2013                     |  Province name                                      |\n",
        "| citycn                       |  City name chinese                                  |\n",
        "| geocode4_corr                |  City code                                          |\n",
        "| CIC                          |   2 digits industry code                            |\n",
        "| IndustryName                 |  CIC industry name                                  |\n",
        "| financial_dep_china          |  Financial dependence industry Chinese Data         |\n",
        "| SO2_emissions_2005           |  SO2 emission in 2005 city level                    |\n",
        "| ttoutput                     |  Total output city-industry-year level              |\n",
        "| tso2                         |  Total SO2 city-industry-year level                 |\n",
        "| so2_intensity_value_added    | Total SO2 over value added city-industry-year level |\n",
        "| tso2_mandate_c               |  Amount of SO2 to reduce city level                 |\n",
        "| avg_ij_o_city_mandate        |  City effort over output                            |\n",
        "| d_avg_ij_o_city_mandate      |  dummy City effort over output                      |\n",
        "| fixed_asset                  |  Total fixed asset city-industry-year level         |\n",
        "| employment                   |  Total employemnt city-industry-year level          |\n",
        "| FE_c_y                       |  pair group city-year                               |\n",
        "| FE_c_i                       |  pair group city industry                           |\n",
        "| FE_i_y                       |  pair group industry-year                           |"
      ],
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "id": "6wYRsIdHh7GI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_df(df,\n",
        "             df_prov_en,\n",
        "             method='value_added',\n",
        "             to_csv=True):\n",
        "    \"\"\"\n",
        "    Merge all data,\n",
        "    df is the credit pollution dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) create city reduction df\n",
        "    city_reduction = city_mandate(df=df,\n",
        "                                  var='tso2',\n",
        "                                  #output = output_2005,\n",
        "                                  move_to_drive=False)['df_ratio_city']\n",
        "\n",
        "    # 2) create city reallocation\n",
        "    # city_reac = reallocation(df_mandate=city_reduction,\n",
        "    #                         distance=distance,\n",
        "    #                         method=real_method,\n",
        "    #                         move_to_drive=False)\n",
        "\n",
        "    city_reac = reallocation(city_mandate=city_reduction,\n",
        "                             output=output_2005,\n",
        "                             city_distance=distance)['reallocation']\n",
        "\n",
        "    # 3) create value added and fixed asset vars\n",
        "    value_added = keepcolumn(df=df_ASIF_raw, method=method)\n",
        "\n",
        "    # 4) create capital over employement\n",
        "    c_l = employment(df=df_ASIF_raw)\n",
        "\n",
        "    # 5) Create gdp per cap\n",
        "    #gdp = gdp_per_cap(df = citydata)\n",
        "\n",
        "    # 6) Merge everything\n",
        "\n",
        "    city_reduction = city_reduction.drop(columns='prov2013')\n",
        "    df_final = pd.merge(df, city_reduction, how='inner',\n",
        "                        on='citycn')\n",
        "    # df_final  = pd.merge(df_final, city_reduction_cod, how = 'inner',\n",
        "    #       on = 'citycn')\n",
        "    df_final = pd.merge(df_final, city_reac, how='inner',\n",
        "                        on='citycn')\n",
        "    df_final = pd.merge(df_final, value_added, how='inner', on=['year',\n",
        "                                                                'geocode4_corr',\n",
        "                                                                'CIC'])\n",
        "    df_final = pd.merge(df_final, c_l, how='inner', on=['year',\n",
        "                                                        'geocode4_corr',\n",
        "                                                        'CIC'])\n",
        "    # df_final = pd.merge(df_final, gdp, how= 'inner', on = ['year',\n",
        "    #                                                     'geocode4_corr'])\n",
        "\n",
        "    df_final['Post'] = np.where(df_final['year'] > 2005, 'yes', 'no')\n",
        "\n",
        "    df_final['year'] = df_final['year'].astype('str')\n",
        "\n",
        "    df_final['FE_c_y'] = pd.factorize(df_final['geocode4_corr'] +\n",
        "                                      df_final['year'])[0]\n",
        "\n",
        "    df_final['FE_c_i'] = pd.factorize(df_final['geocode4_corr'] +\n",
        "                                      df_final['CIC'])[0]\n",
        "\n",
        "    df_final['FE_i_y'] = pd.factorize(df_final['CIC'] +\n",
        "                                      df_final['year'])[0]\n",
        "\n",
        "    df_final = df_final[(df_final['tso2'] != 0)]\n",
        "    df_final = df_final[(df_final['ttoutput'] != 0)]\n",
        "\n",
        "    name = 'so2_intensity_' + method\n",
        "    df_final[name] = df_final['tso2'] / df_final[method]\n",
        "\n",
        "    l_order = ['year', 'Post', 'prov2013', 'citycn', 'geocode4_corr',\n",
        "               'CIC', 'Industry Name', 'financial_dep_china',\n",
        "               'SO2_emissions_2005', 'ttoutput', 'tso2', name,\n",
        "               'target_perc_SO2',\n",
        "               'tso2_mandate_c',\n",
        "               # 'SO2_perc_reduction_c',\n",
        "               # 'perc_target_tso2',\n",
        "               # 'tso2_mandate_c_j',\n",
        "               # 'SO2_perc_reduction_c_j',\n",
        "               # 'dist_tso2_mandate_c_j',\n",
        "               # 'dist_SO2_perc_reduction_c_j',\n",
        "               'avg_ij_o_city_mandate',\n",
        "               # 'w_avg_ij_o_city_mandate',\n",
        "               'd_avg_ij_o_city_mandate',\n",
        "               # 'd_w_avg_ij_o_city_mandate',\n",
        "               # 'avg_ij_city_mandate',\n",
        "               # 'w_avg_ij_city_mandate',\n",
        "               # 'avg_ij_o_perc_city_mandate',\n",
        "               # 'w_avg_ij_o_perc_city_mandate',\n",
        "               # 'avg_ij_perc_city_mandate',\n",
        "               # 'w_avg_ij_perc_city_mandate',\n",
        "               # 'intensity_mandate',\n",
        "               # 'w_avg_ij_city_mandate',\n",
        "               # 'dist_tso2_mandate_c_j',\n",
        "               # 'avg_ij_perc_city_mandate',\n",
        "               # 'avg_ij_o_perc_city_mandate',\n",
        "               # 'w_avg_ij_perc_city_mandate',\n",
        "               # 'dist_SO2_perc_reduction_c_j',\n",
        "               # 'd_avg_ij_city_mandate',\n",
        "               # 'norm_avg_ij_city_mandate',\n",
        "               # 'd_avg_ij_perc_city_mandate',\n",
        "               # 'norm_avg_ij_perc_city_mandate',\n",
        "               #'dist_to_mandate_0', 'dist_to_mandate_1','mean_mandate',\n",
        "               #'city_intensity_ij', 'city_intensity_ij_dist',\n",
        "               # 'norm_city_intensity_ij','norm_city_intensity_ij_dist',\n",
        "               # 'median_mandate',\n",
        "               # 'intensity_ij_dist',\n",
        "               #'norm_intensity_ij_dist', 'intensity_ij', 'norm_intensity_ij',\n",
        "               #'d_intensity_ij' ,\n",
        "               # 'd_city_intensity_ij',\n",
        "               'fixed_asset', 'employment',\n",
        "               #'ratio_cap_labor', 'gdp_cap',\n",
        "               'FE_c_y', 'FE_c_i', 'FE_i_y']\n",
        "\n",
        "    df_final = df_final[l_order].sort_values(by=['year',\n",
        "                                                 'geocode4_corr',\n",
        "                                                 'CIC'])\n",
        "\n",
        "    # Exclude outliers\n",
        "    #df_final['target_perc_SO2'] = df_final['target_perc_SO2'].astype('float')\n",
        "    df_final['year'] = df_final['year'].astype('float')\n",
        "    # df_final = df_final[\n",
        "    #    (df_final['target_perc_SO2'] > 0) &\n",
        "    #    (df_final['temp'] > 0.1) &\n",
        "    #    (df_final['ttoutput'] > 0)]\n",
        "\n",
        "    q95 = df_final['ttoutput'].quantile(.95)\n",
        "    q01 = df_final['ttoutput'].quantile(.01)\n",
        "    df_final = df_final[df_final['ttoutput'] < q95]\n",
        "\n",
        "    df_final = df_final[df_final['ttoutput'] > q01]\n",
        "\n",
        "    # df_final['d_avg_ij_city_mandate_1'] = np.where(df_final['year'] < 2006,\n",
        "    #                                               'No', df_final['d_avg_ij_city_mandate']\n",
        "    #                                               )\n",
        "\n",
        "    # df_final['d_avg_ij_city_mandate_2'] = np.where(df_final['year'] < 2006,\n",
        "    #                                               'Before', df_final['d_avg_ij_city_mandate']\n",
        "    #                                               )\n",
        "\n",
        "    # df_final['d_avg_ij_perc_city_mandate_1'] = np.where(df_final['year'] < 2006,\n",
        "    #                                                    'No', df_final['d_avg_ij_perc_city_mandate']\n",
        "    #                                                    )\n",
        "\n",
        "    # df_final['d_avg_ij_perc_city_mandate_2'] = np.where(df_final['year'] < 2006,\n",
        "    #                                                    'Before', df_final['d_avg_ij_perc_city_mandate']\n",
        "    #                                                    )\n",
        "\n",
        "    #####\n",
        "    unique_pro_ed = df_final[['financial_dep_china',\n",
        "                              'CIC']].drop_duplicates()\n",
        "    med_ed = np.median(unique_pro_ed['financial_dep_china'])\n",
        "    med_ed\n",
        "\n",
        "    df_final['color_ext_dep'] = np.where(\n",
        "        df_final['financial_dep_china'] >\n",
        "        med_ed, \"Above\", \"Below\"\n",
        "    )\n",
        "\n",
        "    unique_pro_target = df_final[['target_perc_SO2',\n",
        "                                  'prov2013']].drop_duplicates()\n",
        "    med_target_SO2 = np.median(unique_pro_target['target_perc_SO2'])\n",
        "\n",
        "    df_final['color_target_SO2'] = np.where(\n",
        "        df_final['target_perc_SO2'] >\n",
        "        med_target_SO2, \"Above\", \"Below\")\n",
        "\n",
        "    # Add loc province\n",
        "\n",
        "    #coast = ['辽宁省', '河北省', '天津市', '山东省', '北京市',\n",
        "    #         '江苏省', '上海市', '浙江省', '福建省',\n",
        "    #         '广东省', '海南省', '广西壮族自治区']\n",
        "\n",
        "    #east = ['黑龙江省', '吉林省', '辽宁省', '河北省', '北京市',\n",
        "    #        '天津市', '山东省', '江苏省', '上海市', '浙江省',\n",
        "    #        '福建省', '广东省']\n",
        "\n",
        "    #central = ['山西省',\n",
        "    #           '安徽省', '江西省', '河南省', '湖北省', '湖南']\n",
        "\n",
        "    #df_final['Coastal'] = np.where(\n",
        "    #    df_final['prov2013'].isin(coast), 'coast', 'notCoast')\n",
        "\n",
        "    #df_final['prov_loc'] = np.where(\n",
        "    #    df_final['prov2013'].isin(east), 'East',\n",
        "    #    np.where(\n",
        "    #        df_final['prov2013'].isin(central), 'Central',\n",
        "    #        'Western'\n",
        "    #    )\n",
        "    #)\n",
        "    \n",
        "    df_final= pd.merge(df_final, df_prov_en, on = 'prov2013',\n",
        "        how = 'inner')\n",
        "    \n",
        "    df_final['year'] = df_final['year'].astype('str')\n",
        "    df_final['year'] = df_final['year'].str.split('.').str[0]\n",
        "\n",
        "    print(df_final.shape)\n",
        "    print(df_final['citycn'].nunique())\n",
        "\n",
        "    ####\n",
        "\n",
        "    if to_csv:\n",
        "        name = 'df_final_china_' + method + '.csv'\n",
        "\n",
        "        df_final.to_csv(\n",
        "            name,\n",
        "            sep=',',\n",
        "            header=True,\n",
        "            index=False,\n",
        "            chunksize=100000,\n",
        "            encoding='utf-8')\n",
        "\n",
        "        #mime_type = \"application/vnd.google-apps.spreadsheet\"\n",
        "        #cdr.upload_file_root(mime_type, name)\n",
        "        #cdr.move_file(file_name=name,\n",
        "        #              folder_name='Pollution_china')\n",
        "\n",
        "    return df_final"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "I_G9HxlcG1lR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_final_credit = df_final_credit.apply(pd.to_numeric,\n",
        "                                        errors='ignore')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "8bxj6OgY1-up"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_final_credit['CIC'] = df_final_credit['CIC'].astype('str')\n",
        "df_final_credit['geocode4_corr'] = df_final_credit['geocode4_corr'].astype('str')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#df_final = merge_df(df = df_final_credit, method = 'sales', to_csv = True)\n",
        "#df_final = merge_df(df = df_final_credit, method = 'output', to_csv = True)\n",
        "df_final = merge_df(df=df_final_credit,\n",
        "                    df_prov_en = df_prov_en,\n",
        "                    method='value_added',\n",
        "                    #real_method = m,\n",
        "                    to_csv=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "ks99MRFBHuO5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Move to BigQuey"
      ],
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "id": "w4Tv-1If1-uy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from GoogleDrivePy.google_authentification import connect_service_local\n",
        "from GoogleDrivePy.google_drive import connect_drive\n",
        "%matplotlib inline\n",
        "pathcredential = '/Users/Thomas/Google Drive/Projects/Data_science/' + \\\n",
        "'Google_code_n_Oauth/Client_Oauth/Google_auth/'\n",
        "\n",
        "scopes = ['https://www.googleapis.com/auth/documents.readonly',\n",
        "            'https://www.googleapis.com/auth/drive',\n",
        "         'https://www.googleapis.com/auth/spreadsheets.readonly']\n",
        "\n",
        "serviceaccount = pathcredential + 'valid-pagoda-132423-c6ac84b41833.json'\n",
        "cs = connect_service_local.connect_service_local(\n",
        "    path_json =pathcredential,\n",
        "    path_service_account = serviceaccount,\n",
        "    scope = scopes)\n",
        "service = cs.get_service()\n",
        "\n",
        "cdr = connect_drive.connect_drive(service, verbose =  False)\n",
        "\n",
        "from GoogleDrivePy.google_platform import connect_cloud_platform\n",
        "project = 'valid-pagoda-132423'\n",
        "service_account = cs.get_storage_client()\n",
        "ccp = connect_cloud_platform.connect_console(project = project, \n",
        "                                             service_account = service_account,\n",
        "                                             colab = False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "fpkVS4Ck1-uy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_name = 'store_data_trade'\n",
        "destination_blob_name = 'Papers/CreditConstraint_SO2_China'\n",
        "source_file_name = 'CreditConstraint_SO2_China.csv'\n",
        "ccp.upload_blob(bucket_name, destination_blob_name,  source_file_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "0fCDYwKl1-u1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SQL_schema = [\n",
        "    ['year', 'INTEGER'],\n",
        "    ['Post', 'STRING'],\n",
        "    ['prov2013', 'STRING'],\n",
        "    ['citycn', 'STRING'],\n",
        "    ['geocode4_corr', 'INTEGER'],\n",
        "    ['CIC', 'INTEGER'],\n",
        "    ['Industry_Name', 'STRING'],\n",
        "    ['financial_dep_china', 'FLOAT'],\n",
        "    ['SO2_emissions_2005', 'FLOAT'],\n",
        "    ['ttoutput', 'FLOAT'],\n",
        "    ['tso2', 'FLOAT'],\n",
        "    ['so2_intensity_value_added', 'FLOAT'],\n",
        "    ['target_perc_SO2', 'FLOAT'],\n",
        "    ['tso2_mandate_c', 'FLOAT'],\n",
        "    ['avg_ij_o_city_mandate', 'FLOAT'],\n",
        "    ['d_avg_ij_o_city_mandate', 'FLOAT'],\n",
        "    ['fixed_asset', 'FLOAT'],\n",
        "    ['employment', 'FLOAT'],\n",
        "    ['FE_c_y', 'INTEGER'],\n",
        "    ['FE_c_i', 'INTEGER'],\n",
        "    ['FE_i_y', 'INTEGER'],\n",
        "    ['color_ext_dep', 'STRING'],\n",
        "    ['color_target_SO2', 'STRING'],\n",
        "    ['Coastal', 'STRING'],\n",
        "    ['prov_loc', 'STRING'],\n",
        "]\n",
        "\n",
        "\n",
        "bucket_gcs = 'store_data_trade/Papers/CreditConstraint_SO2_China/CreditConstraint_SO2_China.csv'\n",
        "ccp.upload_bq_predefined_sql(dataset_name='China', name_table='CreditConstraint_SO2_China',\n",
        "                             bucket_gcs=bucket_gcs, sql_schema=SQL_schema)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "id": "vcTBirb61-u3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "id": "d7mkDMB01-u6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quick Summary Statistics\n",
        "\n",
        "In this section, we make a few plots to see how SO2 evolves at the provincial and industrial level"
      ],
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "heading_collapsed": true,
        "id": "CpDlxf-W1-u6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from DataAnalysisPy.AnalysisPy import QuickDescriptive"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "hidden": true,
        "id": "ZiIqFz_Y1-u7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%javascript\n",
        "IPython.OutputArea.auto_scroll_threshold = 9999;"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "hidden": true,
        "id": "zXQmVcE51-u9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_final['year'] = df_final['year'].astype('str')\n",
        "df_final['CIC'] = df_final['CIC'].astype('str')\n",
        "df_final['geocode4_corr'] = df_final['geocode4_corr'].astype('str')\n",
        "\n",
        "QuickDescriptive.PyAnalysis(dataframe=df_final,\n",
        "                           automatic = True,\n",
        "                           Date = 'year',\n",
        "                            cdr = cdr)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "hidden": true,
        "id": "xP2bQ-zX1-vH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_ts_pollution_f(df, var, move_to_drive = False):\n",
        "  \n",
        "  ###\n",
        "  fig, axarr = plt.subplots(1, 2, figsize=(12, 8))\n",
        "  temp = df.groupby(['year']).agg({var:['sum', 'mean']})\n",
        "  temp.columns = temp.columns.droplevel()\n",
        "  temp.columns = [ var + '_sum', var + '_mean']\n",
        "\n",
        "  g = sns.lineplot(y= var + '_sum', x= temp.index, data=temp , ax=axarr[0])\n",
        "  g = sns.lineplot(y= var + '_mean', x= temp.index, data=temp, ax=axarr[1])\n",
        "  \n",
        "  if move_to_drive:\n",
        "    name = \"Sum_\" + var  + \"_pollution.png\"\n",
        "    g.get_figure().savefig(name)\n",
        "    mime_type = \"image/png\"\n",
        "    cdr.upload_file_root(mime_type, name)\n",
        "    cdr.move_file(file_name = name,\n",
        "                    folder_name = 'Coda')\n",
        "    os.remove(name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "hidden": true,
        "id": "ojrV4tgPzFwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_parallele(df, Y, group, move_to_drive = False):\n",
        "  \n",
        "  \n",
        "  df_var = df.copy()\n",
        "  \n",
        "    \n",
        "  test = df_var.groupby(['year',\n",
        "                         group])[\n",
        "      Y].mean().reset_index()\n",
        "  \n",
        "  df_temp = pd.melt(test, id_vars=['year', group],\n",
        "        value_vars=[Y],\n",
        "        value_name=Y)\n",
        "  \n",
        "  g = sns.lineplot(x=\"year\", y=Y, hue = group,  data=df_temp)\n",
        "  \n",
        "  if move_to_drive:\n",
        "    name = \"mean_\" + Y  + \"_target_pollution.png\"\n",
        "    g.get_figure().savefig(name)\n",
        "    mime_type = \"image/png\"\n",
        "    cdr.upload_file_root(mime_type, name)\n",
        "    cdr.move_file(file_name = name,\n",
        "                    folder_name = 'Coda')\n",
        "    os.remove(name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "hidden": true,
        "id": "SLm6T8eAzdz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multiple_group(df, var2, Y, move_to_drive = False):\n",
        "\n",
        "  df_var = df.copy()\n",
        "\n",
        "  test = df_var.groupby(['year', 'color_ext_dep', var2\n",
        "                         ])[\n",
        "      Y].mean().reset_index()  \n",
        "  \n",
        "  sns.set(style=\"ticks\", color_codes=True, font_scale = .65)\n",
        "  g = sns.FacetGrid(test, col = 'color_ext_dep',\n",
        "                  row=var2,\n",
        "                 height=4)\n",
        "  g = g.map(plt.plot, \"year\",Y)\n",
        "  \n",
        "  if move_to_drive:\n",
        "    name = \"mean_\" + Y  + \"_CIC_target_pollution.png\"\n",
        "    g.savefig(name)\n",
        "    mime_type = \"image/png\"\n",
        "    cdr.upload_file_root(mime_type, name)\n",
        "    cdr.move_file(file_name = name,\n",
        "                    folder_name = 'Coda')\n",
        "    os.remove(name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "hidden": true,
        "id": "kCmrRWGpzfnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CIC_province(Year , Y, province, move_to_drive = False):\n",
        "  \n",
        "  temp = df_pro.copy()\n",
        "  \n",
        "  temp = temp[temp['prov2013'] == province]\n",
        "  temp = temp.groupby(['Post', 'CIC'])[Y,\n",
        "                              'financial_dep_china'].mean().reset_index()\n",
        "  name = 'ln_'+ Y\n",
        "  temp[name] = np.log(temp[Y])\n",
        "  g = sns.lmplot(x=\"financial_dep_china\",\n",
        "           y=name,\n",
        "           hue = 'Post',\n",
        "           ci=None,\n",
        "           data=temp)\n",
        "  plt.title(Year + \"_\" + province)\n",
        "  \n",
        "  if move_to_drive:\n",
        "    name = Year + \"_\" + province + \"_CIC_province.png\"\n",
        "    g.get_figure().savefig(name)\n",
        "    mime_type = \"image/png\"\n",
        "    cdr.upload_file_root(mime_type, name)\n",
        "    cdr.move_file(file_name = name,\n",
        "                    folder_name = 'Coda')\n",
        "    os.remove(name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "hidden": true,
        "id": "AIXvEKmLzkQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dummy(df):\n",
        "  \n",
        "  temp = df.copy()\n",
        "  temp['financial_dep_china'] = temp['financial_dep_china'].astype('float') \n",
        "  for  var in ['tso2']:\n",
        "    print(var)\n",
        "  \n",
        "    unique_pro_ed = temp[['financial_dep_china',\n",
        "                      'CIC']].drop_duplicates() \n",
        "  \n",
        "    med_ed = np.median(unique_pro_ed['financial_dep_china'])\n",
        "\n",
        "    temp['color_ext_dep'] = np.where(\n",
        "                            temp['financial_dep_china'] > \n",
        "                            med_ed,\"Above\", \"Below\")\n",
        "  \n",
        "  #### 9. Get median city mandate\n",
        "  \n",
        "    if var == 'tso2':\n",
        "      obj = 'tso2_mandate_c'\n",
        "      target = 'target_perc_SO2'\n",
        "      mandate = city_reduction\n",
        "    else:\n",
        "      obj = 'tCOD_mandate_c'\n",
        "      target = 'target_perc_COD'\n",
        "      mandate = city_reduction_cod\n",
        "  \n",
        "    med_target_c = np.median(mandate[obj])\n",
        "  \n",
        "    #df = pd.merge(df, mandate, how = 'left', on = 'city')\n",
        "  \n",
        "    name_c = 'color_c_' + obj\n",
        "    \n",
        "    #print(list(df))\n",
        "  \n",
        "    temp[name_c] = np.where(\n",
        "      temp[obj] > med_target_c, \"Above\", \"Below\")\n",
        "  \n",
        "  ### 10. Get Median Province\n",
        "    temp[target] = temp[target].astype('float')\n",
        "    unique_pro_target = temp[[target, 'prov2013']].drop_duplicates() \n",
        "    med_target_ = np.median(unique_pro_target[target])\n",
        "\n",
        "    name = 'color_' + target\n",
        "    temp[name] = np.where(\n",
        "      temp[target] > med_target_, \"Above\", \"Below\")\n",
        "    \n",
        "  return temp"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "hidden": true,
        "id": "fpoyAoNPkS-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_graph = get_dummy(df_final)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "hidden": true,
        "id": "A-Gc0l2-kE0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_ts_pollution_f(df_graph, var = 'tso2', move_to_drive = False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "hidden": true,
        "id": "NXJPf8F1mFMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_parallele(df = df_graph,\n",
        "               Y = 'tso2',\n",
        "               group = 'color_target_perc_SO2', \n",
        "               move_to_drive = False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "hidden": true,
        "id": "McmkUEVsyhXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_parallele(df = df_graph,\n",
        "               Y = 'tso2',\n",
        "               group = 'color_c_tso2_mandate_c', \n",
        "               move_to_drive = False)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "hidden": true,
        "id": "SaUh6cuJyhtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_parallele(df = df_graph,\n",
        "               Y = 'tso2',\n",
        "               group = 'color_ext_dep', \n",
        "               move_to_drive = False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "hidden": true,
        "id": "tX5wz1gVzAEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " multiple_group(df = df_graph, \n",
        "                var2 = 'color_target_perc_SO2',\n",
        "                Y = 'tso2',\n",
        "                move_to_drive = False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "Collapsed": "false",
        "colab": {},
        "colab_type": "code",
        "hidden": true,
        "id": "1SznQaQezBER"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "RZkqiALo26n7",
        "Pgao7gQU7U-A",
        "3NfGLp6d8Ja7",
        "7x6cJKX6Bug5",
        "_NNvB8O3Bwgz",
        "dA9c_d93FWy_",
        "7BfGDI1FE-O9",
        "MK2r_DMkWCN5",
        "s89GYy6Y0wJt"
      ],
      "name": "01_Preparation_dataset_Chinadata.ipynb",
      "private_outputs": true,
      "provenance": [],
      "version": "0.3.2"
    },
    "hide_input": false,
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "toc": {
      "toc_position": {},
      "skip_h1_title": false,
      "number_sections": true,
      "title_cell": "Table of Contents",
      "toc_window_display": true,
      "base_numbering": 1,
      "toc_section_display": true,
      "title_sidebar": "Contents",
      "toc_cell": false,
      "nav_menu": {},
      "sideBar": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "0.15.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}